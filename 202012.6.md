# **一.卷积神经网络：**

## **1.概述：**

1. 卷积神经网络输入与输出维数相同。

## **2.卷积：**

1. 目的：从输入的图像中提取特征，同时保留像素的空间关系。

2. 滤波器/核(filter/kernel):在图像矩阵上滑动点成计算得到一个矩阵，被称为“卷积特征”。其作用就是“特征检测”。

3. 训练中会学习这些滤波器的值，使用的滤波器越多，提取的图像特征就越多，整个神经网络在对知图片识别就越好。

4. 特征图片的大小由三个参数决定:(1)深度：对应filter个数。(2)步长（可以filter中心为标准）：滑动filter时的像素数，步长越大特征图越小。
(3)零填充：对图片的四周进行填充。作用：不希望图片大小发生变化；原始图像可能除不尽，那么剩下的像素点就会被舍弃。不用零填充的叫做严格卷积。

5. 若有多个通道，则分开计算，对每个通道求feature map后对应位置相加得到最终的feature map。

6. 若有多个filter，则下一输入层有多个通道。

7. Full模式:从原始图像数据的第一个像素点开始，其余补0，即第一次卷积计算中窗口只有第一个像素点。

8. Same模式：filter中心与输入的原始图像数据完全相交，需要补0。

9. Valid模式：filter和输入图像完全相交：即卷积过程中filter始终不超过输入图像，因此不需要补0。

## **二.非线性激活函数：**

1. 一般常用ReLU函数，可以解决梯度消失问题(Sigmoid函数最大梯度为0.25，深层神经网络中根据链式法则，梯度会越来越小，从造成梯度消失)。

2. ReLU函数会有神经元死亡的问题，可以使用Leaky ReLU，PReU(采用自带动量更新方式)，elu（输出均值接近0，需进行幂计算，计算量大）。

## **三.池化操作：**

1. 又称亚采样，下采样，降低特征图的维度，但保持了大部分重要的信息，一定程度上防止过拟合。

2. 一般来说，卷积层之间会周期性地插入池化层。

3. 常用操作有最大池化：能更好保留纹理特征。

4. 平均池化：往往能保留整体数据的特征，较好的突出背景信息。

5. 随机池化：随机池化中元素值大的被选中的概率也大，但不是像最大池化总是取最大值。随机池化一方面最大化地保证了Max值的取值，一方面又确保了不会完全是max值起作用，造成过度失真。除此之外，其可以在一定程度上避免过拟合。

6. 重叠池化：类似卷积运算中的滑动，而不是将数据分块。

