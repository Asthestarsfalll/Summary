# **学习报告**

## **Pandas**

### **一.概述：**

1.核心数据结构为一维的Series和二维的DataFrame。

2.Series是带有标签的同构类型数组，DataFrame是一个二维的表结构，可以看作由多个Series组成。

### **二.数据读取和储存：**

1. csv文件读取：read_csv：
    1. filepath_or_buffer：配置所需读取 CSV 文件的路径。
    2. sep：配置 CSV 文件的列分隔符，默认是逗号“,”。
    3. delimiter：可选配置，作为 sep 配置分隔符的别名。
    4. delim_whitespace：配置是否用空格来作为列分隔符。如果设置为 True，那么 sep 配置参数就不起作用了。
    5. header：配置用行数来作为列名，默认配置成自动推断。
    6. names：配置列名，如果所读取的 CSV 文件没有表头，那么需要配置header=None，否则会将第一行数据作为对应的列名。
    7. usecols：当只需要读取 CSV 文件中的部分数据时，可以使用 usecols 来指定读取列名以获取数据。
    8. dtype：配置所读取数据的类型。
    9. encoding：配置文件的编码方式，一般使用 UTF-8 或者 GBK。

2. csv文件储存：to_csv:
    1. path_or_buf：配置 CSV 文件的保存路径。
    2. sep：配置保存文件的分隔符，默认是逗号“,”。
    3. na_rep：配置空值补全的值，默认用空格代替。
    4. float_format：配置将浮点数格式化成字符串类型。
    5. columns：配置需要写入列的列名，如果不配置，则默认从第 1 列开始写入。
    6. header：配置是否写入列名，默认是需要写的。
    7. index：配置是否写入行名，默认是需要写的。
    8. index_label：配置用来作为列索引的列，默认是没有的。
    9. mode：配置写入模式，默认是 W。
    10. encoding：配置编码格式，该配置只针对 Python 3 以前的版本。
    11. line_terminator：配置每行的结束符，默认使用“\n”。
    12. quotin：配置 CSV 的引用规则。
    13. quotechar：配置用来作为引用的字符，默认是空格。
    14. chunksize：配置每次写入的行数。
    15. tuplesize_cols：配置写入 list 的格式，默认以元组的方式写入。
    16. date_format：配置时间数据的格式

3. JSON文件读取：read_json：
    1. filepath_or_buffer：配置有效的 JSON 字符串、JSON 文件的路径或者数据接口。数据接口可以是一个 URL 地址。
    2. type：配置将读取的数据生成 Series 还是 DataFrame，默认是 DataFrame。

4. JSON文件储存：to_json:
    1. path_or_buf：配置 JSON 数据保存的路径或者写入的内存区域。
    2. date_format：配置时间数据的格式，epoch 表示配置成时间戳的格式，iso表示配置成 ISO 8601 的格式。
    3. double_precision：配置小数点后保留的位数，默认是 10 位。
    4. force_ascii：配置是否强制将 String 转码成 ASCII，默认强制进行转码。
    5. date_unit：配置时间数据的格式，可以实现精确到秒级或毫秒级。

### **三.数据查看：**

1. .shape[]，0为行数，1位列数。

2. .head() ，默认返回数据前五行，可传参。

3. .tail()，默认返回数据后五行，可传参。

4. .describe()， 查看数据汇总统计。

5. .info()，查看数据概况。

6. .columns()，查看列名。

7. .mean()，查看各列平均值。

### **四.数据选取：**

1. 通过列名索引，用【】或者在末尾加.name

2. .loc传递的是索引的名称，.iloc传递的是索引的相对位置。

### **五.数据处理：**

1. 数据合并:
    1. pd.concat()按行拼接。
    2. pd.merge()按列拼接。
    3. 末尾加.append()增加某列数据。

2. 数据清理:
    1.使用.isnull()检查数据是否为空置，此时末尾再加.sum()可进行空值的统计。
    2. .fillna(n)，n为替换空格的值。
    3. .dropna()删除包含空值的行和列。

3. 数据处理：
    1. .astype()转换数据类型。
    2. .colums.value_counts()可以统计唯一值的个数。
    3. .sort_value(by=colname,ascending=True)可以用于序列排序。

## **PIL**

### **一.图像读写：**

1. Image.open()打开图像文件和读取图像数据。

2. 可以读取tar文件。

3. Image.save()将图片保存为JPEG格式。

### **二.图像编辑：**

1. 生成缩略图：im.thumbnai((xx,xx))。

2. 查询图像格式：
    1. im.format()查询图片格式。
    2. im.size()查询图片尺寸。
    3. im.mode()查询图片色彩模式(RGB、L)。

3. 图像截取：im.crop()。

4. 图像尺寸变换：
    1. resize((xx,xx))。
    2. im.rotate()图片旋转功能。

5. 像素变换：
    1. .convert()可对图像进行二值化处理。
    2. 像素对比度调节：im.filter()。

## **神经网络拾遗：**

1. 每层节点数实际上相当于输入数据特征值的个数。

2. 矩阵运算相当于同时对一层内的全部神经元进行运算。

3. ReLU函数相当于模拟人类神经元，因为当输入数据小于0时，输出便是为0，相当于人脑神经网络中并不是所有神经元都被激活，这些神经元暂时性休眠。(过多的神经元休眠被称为死亡，可以用Maxout激活函数，拥有ReLU的所有优点且不会死亡。)

4. softmax函数常在多分类中使用，若种类为2，则退化为sigmoid函数.

5. 若参数初始化为0，或者其他统一的常量，会导致后面的激活单元具有相同的值，所有的单元相同意味着它们都在计算同一特征，网络变得跟只有一个隐含层节点一样，这使得神经网络失去了学习不同特征的能力！

6. 全连接通过权重矩阵实现，每个权重矩阵连接两层，矩阵中元素个数为两层节点数之积。权重矩阵的行数为下一层的节点数，列数为本层节点数。数据将通过权重矩阵将特征值映射，而本身数量不改变。

7. 权重矩阵的行分别代表本层的神经元，列分别代表下层神经元，第i行第j列的参数即表示本层第j个神经元到下层第i个神经元的连接权重。

8. 输入层没有权重矩阵偏置和激活函数，其数据当做下一层的输入。

9. 交叉熵损失函数和sigmoid或softmax函数符合是凸函数，存在全局最优解。且误差与最后一层的梯度与激活函数的导数的无关，训练得会更快。

10. 神经网络从有限的观测数据中学习(猜出)具有一般性的规律，并利用这些规律对未知数据进行预测。

11. 全连接神经网络的特征提取：不断地更新权重可以使某些重要的数据得到与其相匹配的地位，使另一些不重要的数据失去影响预测结果的能力，这是不是也是一种特征提取。

12. 全连接神经网络主要起分类作用，即调整权重使得更重要的特征拥有更大的话语权————即对预测结果的影响更大。

13. 泛化能力可以用来衡量一个神经网络训练的好坏程度，体现的是神经网络对新数据的适应性。

14. 在我看来，无论是分类问题还是回归问题，都是利用神经网络去拟合一个函数。对于分类问题而言，神经网络拟合的函数实际上是客观上存在的一个最优的函数，这个函数与所有权重偏置和神经网络的层数节点数都有关系，最小化误差函数的同时还可以保持优秀的泛化能力，然而由于算力原因，几乎不可能找到最优解，因此需要在最小化误差函数和保持泛化能力之间寻找平衡，同时也有了过拟合欠拟合一说。

15. 

## **TensorFlow 2.0**

### **一.简介：**

1. 