# **BP神经网络**

## **一. 大概内容：**

1. 初始化参数：需随机初始化权值，破坏网络的对称性，若全都初始化为0，每层计算结果相同，则不能达到训练的效果。

2. 向前线性传播：输入神经网络的数据一般为矩阵，需要经过与权重的计算输给激活函数。

3. 计算激活值：由前一步得到的值输入激活函数得到激活值再继续向前传播。

4. 计算误差：将最终所得到的结果与真实值进行对比，得到误差，并且进行反向传播。

5. 反向传播：运用链式法则将误差进行反向传播。

6. 更新权重和偏置量。

7. 理论上两层神经网络可以拟合任何函数，因为第一层的矩阵会改变数据的空间，使得不可被线性可分的数据变为线性可分。
   参考：https://blog.csdn.net/weixin_30271335/article/details/96056235?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param

8. BP神经网络：核心是反向传播，相比于传统正向通过链式法则计算梯度的方法，避免了过多冗余的重复计算，有效的解决了计算量过大的问题。

## **二. 初始化参数：**

1. 若为没有隐藏层的最简单神经网络则可以初始化为0。

2. 一般初始化的值很小。

3. 初始化的方法：https://blog.csdn.net/qq_26898461/article/details/50996507

4. 合适的参数初始化可以使训练得更快。

## **三. 向前线性传播：**

1. 输入的数据一般为矩阵，需要注意矩阵乘法的运算法则。

2. 偏执参数b: 可以使激活函数向左向右移动，更好地拟合数据。

## **四. 激活函数：**

1. 常见的激活函数有：ReLU，sigmoid，TanH。

2. 激活函数都为非线性，否则失去了多层神经网络的效果，不能进行非线性拟合。

3. sigmoid函数的导数：e^-z/(1+e^-z)  =  1/(1+e^-z)*(1-1/(1+e^-z))  =  sigmoid（1-sigmoid）

4. ReLU函数的导数：1(x>0);0(x<=0);

5. TanH：(e^x-e^-x)/(e^x+e^-x)，其导数：1-TanH^2(有文献认为TanH总比sigmoid函数好)

## **五. 反向传播 ：**

1. https://blog.csdn.net/weixin_38347387/article/details/82936585?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522160620020319724839257429%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=160620020319724839257429&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-82936585.first_rank_ecpm_v3_pc_rank_v2&utm_term=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&spm=1018.2118.3001.4449

2. 链式法则：复合函数的导数将是构成复合这有限个函数在相应点的 导数的乘积，就像锁链一样一环套一环，故称链式法则。

3. 反向传播：对输出层到激活函数，激活函数到线性传播，线性传播到前一层的权重分别求导乘积，求出梯度。

4. 注意每次是对谁求导。

5. 隐含层的之间反向传播时需要计算与该神经元相连的各个神经元所产生的误差并相加。

## **六. 更新参数：**

1. 运用梯度下降法进行更新。