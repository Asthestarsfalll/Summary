# **一.反向传播：**

1. 此梯度是相对于最终的误差而言的。

2. 反向传播的理论基础是链式法则，反向链式法则可以减少用正向链式法则时的重复计算。

3. 每一步偏导都是正向传播时的输出对输入。

4. 由3，需要将每次正向传播时的参数储存，便于求反向传播求梯度。

5. 矩阵对矩阵求偏导，雅克比矩阵，将两个矩阵分别按照横向和纵向展开，再将对应位置填上，类似于矩阵乘法的排序.

6. 矩阵求导网站：http://www.matrixcalculus.org/

# **二.神经网络拾遗：**

1. 每层节点数实际上相当于输入数据特征值的个数。

2. 矩阵运算相当于同时对一层内的全部神经元进行运算。

3. ReLU函数相当于模拟人类神经元，因为当输入数据小于0时，输出便是为0，相当于人脑神经网络中并不是所有神经元都被激活，这些神经元暂时性休眠。(过多的神经元休眠被称为死亡，可以用Maxout激活函数，拥有ReLU的所有优点且不会死亡。)

4. softmax函数常在多分类中使用，若种类为2，则退化为sigmoid函数.

5. 若参数初始化为0，或者其他统一的常量，会导致后面的激活单元具有相同的值，所有的单元相同意味着它们都在计算同一特征，网络变得跟只有一个隐含层节点一样，这使得神经网络失去了学习不同特征的能力！

# **三.准确率计算：**

1. 若标签仅为0,1，可以用np.mean(np.abs(label_p-label))。

2. 对任意标签，可用np.mean(label_p==label)。